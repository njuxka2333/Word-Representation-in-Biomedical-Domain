{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Representation in Biomedical Domain\n",
    "\n",
    "Before you start, please make sure you have read this notebook. You are encouraged to follow the recommendations but you are also free to develop your own solution from scratch. \n",
    "\n",
    "## Marking Scheme\n",
    "\n",
    "- Biomedical imaging project: 40%\n",
    "    - 20%: accuracy of the final model on the test set\n",
    "    - 20%: rationale of model design and final report\n",
    "- Natural language processing project: 40%\n",
    "    - 30%: completeness of the project\n",
    "    - 10%: final report\n",
    "- Presentation skills and team work: 20%\n",
    "\n",
    "\n",
    "This project forms 40\\% of the total score for summer/winter school. The marking scheme of each part of this project is provided below with a cap of 100\\%.\n",
    "\n",
    "You are allowed to use open source libraries as long as the libraries are properly cited in the code and final report. The usage of third-party code without proper reference will be treated as plagiarism, which will not be tolerated.\n",
    "\n",
    "You are encouraged to develop the algorithms by yourselves (without using third-party code as much as possible). We will factor such effort into the marking process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Prerequisites \n",
    "\n",
    "Recommended environment\n",
    "\n",
    "- Python 3.7 or newer\n",
    "- Free disk space: 100GB\n",
    "\n",
    "Download the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 (20%): Parse the Data\n",
    "\n",
    "The JSON files are located in two sub-folders in `document_parses`. You will need to scan all JSON files and extract text (i.e. `string`) from relevant fields (e.g. body text, abstract, titles).\n",
    "\n",
    "You are encouraged to extract full article text from body text if possible. If the hardware resource is limited, you can extract from abstract or titles as alternatives. \n",
    "\n",
    "Note: The number of JSON files is around 425k so it may take more than 10 minutes to parse all documents.\n",
    "\n",
    "For more information about the dataset: https://www.semanticscholar.org/cord19/download\n",
    "\n",
    "Recommended output:\n",
    "\n",
    "- A list of text (`string`) extracted from JSON files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################\n",
    "import os\n",
    "import json\n",
    "import string\n",
    "\n",
    "#the function for eliminating punctuations\n",
    "def remove_punctuation(text, keep_characters=\"'-\"):\n",
    "    punctuation = ''.join([char for char in string.punctuation if char not in keep_characters])\n",
    "    translator = str.maketrans('', '', punctuation)\n",
    "    return text.translate(translator)\n",
    "\n",
    "#the function for etracting text from json files where titles and main bodies are stored respectively\n",
    "def extract_text_from_json(directory):\n",
    "    text_data = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith('.json'):\n",
    "            with open(os.path.join(directory, filename), 'r') as file:\n",
    "                full_text_dict = json.load(file)\n",
    "                element = full_text_dict['metadata']['title']\n",
    "                for paragraph_dict in full_text_dict['body_text']:\n",
    "                    element += paragraph_dict['text']\n",
    "                element = remove_punctuation(element,keep_characters=\"'-\")\n",
    "                text_data.append(element)\n",
    "    return text_data\n",
    "\n",
    "directory1 = 'document_parses/pdf_json'\n",
    "directory2 = 'document_parses/pmc_json'\n",
    "text_data = extract_text_from_json(directory1) +extract_text_from_json(directory2)\n",
    "\n",
    "\n",
    "\n",
    "###################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 (30%): Tokenization\n",
    "\n",
    "Traverse the extracted text and segment the text into words (or tokens).\n",
    "\n",
    "The following tracks can be developed in independentely. You are encouraged to divide the workload to each team member.\n",
    "\n",
    "Recommended output:\n",
    "\n",
    "- Tokenizer(s) that is able to tokenize any input text.\n",
    "\n",
    "Note: Because of the computation complexity of tokenizers, it may take hours/days to process all documents. Which tokenizer is more efficient? Any idea to speedup?\n",
    "\n",
    "### Track 2.1 (10%): Use split()\n",
    "\n",
    "Use the standard `split()` by Python.\n",
    "\n",
    "### Track 2.2 (10%): Use NLTK or SciSpaCy\n",
    "\n",
    "NLTK tokenizer: https://www.nltk.org/api/nltk.tokenize.html\n",
    "\n",
    "SciSpaCy: https://github.com/allenai/scispacy\n",
    "\n",
    "Note: You may need to install NLTK and SpaCy so please refer to their websites for installation instructions.\n",
    "\n",
    "### Track 2.3 (10%): Use Byte-Pair Encoding (BPE)\n",
    "\n",
    "Byte-Pair Encoding (BPE): https://huggingface.co/transformers/tokenizer_summary.html\n",
    "\n",
    "Note: You may need to install Huggingface's transformers so please refer to its website for installation instructions.\n",
    "\n",
    "### Track 2.4 (Bonus +5%): Build new Byte-Pair Encoding (BPE)\n",
    "\n",
    "This track may be dependent on track 2.3.\n",
    "\n",
    "The above pre-built tokenization methods may not be suitable for biomedical domain as the words/tokens (e.g. diseases, sympotoms, chemicals, medications, phenotypes, genotypes etc.) can be very different from the words/tokens commonly used in daily life. Can you build and train a new BPE model for biomedical domain in particular?\n",
    "\n",
    "### Open Question (Optional):\n",
    "\n",
    "- What are the pros and cons of the above tokenizers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################\n",
    "# 1.simple split\n",
    "split_tokens = [text.split() for text in text_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. NLTK Tokenization\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk_tokens = [word_tokenize(text) for text in text_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.Scispacy for biomedical text\n",
    "import spacy\n",
    "\n",
    "\n",
    "nlp = spacy.load(\"en_core_sci_sm\")  \n",
    "nlp.max_length = 2000000  \n",
    "\n",
    "def preprocess_biomedical_text(text):\n",
    "    doc = nlp(text)\n",
    "    tokens = []\n",
    "    seen_words = set() \n",
    "    for entity in doc.ents:\n",
    "        tokens.append(entity.text)\n",
    "        seen_words.update(entity.text.split())\n",
    "    for token in doc:  \n",
    "        if token.ent_iob_ == \"O\" and token.text not in seen_words:  \n",
    "            tokens.append(token.text)  \n",
    "    return tokens  \n",
    "\n",
    "scispacy_tokens = [preprocess_biomedical_text(text) for text in text_data]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Byte-Pair Encoding (BPE) Tokenization\n",
    "import sentencepiece as spm\n",
    "def train_bpe(text_data, model_prefix='bpe_model'):\n",
    "     with open('corpus.txt', 'w') as f:\n",
    "         for text in text_data:\n",
    "             f.write(text + '\\n')\n",
    "     spm.SentencePieceTrainer.Train(f'--input=corpus.txt --model_prefix={model_prefix} --vocab_size=100')\n",
    "\n",
    "train_bpe(text_data)\n",
    "sp = spm.SentencePieceProcessor(model_file='bpe_model.model')\n",
    "bpe_tokens = [sp.encode_as_pieces(text) for text in text_data]\n",
    "###################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3 (30%): Build Word Representations\n",
    "\n",
    "Build word representations for each extracted word. If the hardware resource is limited, you may limit the vocabulary size up to 10k words/tokens (or even smaller) and the dimension of representations up to 256.\n",
    "\n",
    "The following tracks can be developed independently. You are encouraged to divide the workload to each team member.\n",
    "\n",
    "### Track 3.1 (15%): Use N-gram Language Modeling\n",
    "\n",
    "N-gram Language Modeling is to predict a target word by using `n` words from previous context. Specifically,\n",
    "\n",
    "$P(w_i | w_{i-1}, w_{i-2}, ..., w_{i-n+1})$\n",
    "\n",
    "For example, given a sentence, `\"the main symptoms of COVID-19 are fever and cough\"`, if $n=7$, we use previous context `[\"the\", \"main\", \"symptoms\", \"of\", \"COVID-19\", \"are\"]` to predict the next word `\"fever\"`.\n",
    "\n",
    "More to read: https://web.stanford.edu/~jurafsky/slp3/3.pdf\n",
    "\n",
    "Recommended outputs:\n",
    "\n",
    "- A fixed vector for each word/token.\n",
    "\n",
    "### Track 3.2 (15%): Use Skip-gram with Negative Sampling\n",
    "\n",
    "In skip-gram, we use a central word to predict its context. Specifically,\n",
    "\n",
    "$P(w_{c-m}, ... w_{c-1}, w_{c+1}, ..., w_{c+m} | w_c)$\n",
    "\n",
    "As the learning objective of skip-gram is computational inefficient (summation of entire vocabulary $|V|$), negative sampling is commonly applied to accelerate the training.\n",
    "\n",
    "In negative sampling, we randomly select one word from the context as a positive sample, and randomly select $K$ words from the vocabulary as negative samples. As a result, the learning objective is updated to\n",
    "\n",
    "$L = -\\log\\sigma(u^T_{t} v_c) - \\sum_{k=1}^K\\log\\sigma(-u^T_k v_c)$, where $u_t$ is the vector embedding of positive sample from context, $u_k$ are the vector embeddings of negative samples, $v_c$ is the vector embedding of the central word, $\\sigma$ refers to the sigmoid function.\n",
    "\n",
    "More to read http://web.stanford.edu/class/cs224n/readings/cs224n-2019-notes01-wordvecs1.pdf section 4.3 and 4.4\n",
    "\n",
    "Recommended outputs:\n",
    "\n",
    "- A fixed vector for each word/token.\n",
    "\n",
    "### Track 3.3 (Bonus +5%): Use Contextualised Word Representation by Masked Language Model (MLM)\n",
    "\n",
    "BERT introduces a new language model for pre-training named Masked Language Model (MLM). The advantage of MLM is that the word representations by MLM will be contextualised.\n",
    "\n",
    "For example, \"stick\" may have different meanings in different context. By N-gram language modeling and word2vec (skip-gram, CBOW), the word representation of \"stick\" is fixed regardless of its context. However, MLM will learn the representation of \"stick\" dynamatically based on context. In other words, \"stick\" will have different representations in different context by MLM.\n",
    "\n",
    "More to read: http://jalammar.github.io/illustrated-bert/ and https://arxiv.org/pdf/1810.04805.pdf\n",
    "\n",
    "Recommended outputs:\n",
    "\n",
    "- An algorithm that is able to generate contextualised representation in real time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################\n",
    "# 1. N-gram Model\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "\n",
    "phrases = Phrases(bpe_tokens, min_count=1, threshold=2)\n",
    "bigram = Phraser(phrases)\n",
    "ngram_tokens = [bigram[tokens] for tokens in bpe_tokens]\n",
    "\n",
    "###################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.Skip-gram with Negative Sampling\n",
    "skipgram_model = Word2Vec(sentences=bpe_tokens, vector_size=100, window=5, sg=1, negative=10, min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Masked Language Model (BERT or similar)\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def get_bert_embeddings(text):\n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True)\n",
    "    outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1).detach().numpy()\n",
    "\n",
    "bert_embeddings = [get_bert_embeddings(text) for text in text_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import cached_path  \n",
    "  \n",
    "# 清除缓存  \n",
    "cache_dir = cached_path('bert-base-uncased')  \n",
    "if cache_dir:  \n",
    "    import shutil  \n",
    "    shutil.rmtree(cache_dir)  \n",
    "  \n",
    "# 尝试重新加载  \n",
    "from transformers import BertTokenizer, BertModel  \n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')  \n",
    "model = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4 (20%): Explore the Word Representations\n",
    "\n",
    "The following tracks can be finished independently. You are encouraged to divide workload to each team member.\n",
    "\n",
    "### Track 4.1 (5%): Visualise the word representations by t-SNE\n",
    "\n",
    "t-SNE is an algorithm to reduce dimentionality and commonly used to visualise high-dimension vectors. Use t-SNE to visualise the word representations. You may visualise up to 1000 words as t-SNE is highly computationally complex.\n",
    "\n",
    "More about t-SNE: https://lvdmaaten.github.io/tsne/\n",
    "\n",
    "Recommended output:\n",
    "\n",
    "- A diagram by t-SNE based on representations of up to 1000 words.\n",
    "\n",
    "### Track 4.2 (5%): Visualise the Word Representations of Biomedical Entities by t-SNE\n",
    "\n",
    "Instead of visualising the word representations of the entire vocabulary (or 1000 words that are selected at random), visualise the word representations of words which are biomedical entities. For example, fever, cough, diabetes etc. Based on the category of those biomedical entities, can you assign different colours to the entities and see if the entities from the same category can be clustered by t-SNE? For example, sinusitis and cough are both respirtory diseases so they should be assigned with the same colour and ideally their representations should be close to each other by t-SNE. Another example, Alzheimer and headache are neuralogical diseases which should be assigned by another colour.\n",
    "\n",
    "Examples of biomedial ontology: https://www.ebi.ac.uk/ols/ontologies/hp and https://en.wikipedia.org/wiki/International_Classification_of_Diseases\n",
    "\n",
    "Recommended output:\n",
    "\n",
    "- A diagram with colours by t-SNE based on representations of biomedical entities.\n",
    "\n",
    "### Track 4.3 (5%): Co-occurrence\n",
    "\n",
    "- What are the biomedical entities which frequently co-occur with COVID-19 (or coronavirus)?\n",
    "\n",
    "Recommended outputs:\n",
    "\n",
    "- A sorted list of biomedical entities and description on how the entities are selected and sorted.\n",
    "\n",
    "### Track 4.4 (5%): Semantic Similarity\n",
    "\n",
    "- What are the biomedical entities which have closest semantic similarity COVID-19 (or coronavirus) based on word representations?\n",
    "\n",
    "Recommended outputs:\n",
    "\n",
    "- A sorted list of biomedical entities and description on how the entities are selected and sorted.\n",
    "\n",
    "### Open Question (Optional): What else can you discover?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################\n",
    "#1. Visualise the word representations by t-SNE\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "mpl.rcParams['text.usetex'] = False\n",
    "\n",
    "vocab = list(skipgram_model.wv.index_to_key)\n",
    "embeddings = np.array([skipgram_model.wv[word] for word in vocab])\n",
    "selected_indices = np.random.choice(len(vocab), 500, replace=False)\n",
    "\n",
    "vocab = [vocab[i] for i in selected_indices]\n",
    "embeddings = embeddings[selected_indices]\n",
    "\n",
    "tsne = TSNE(n_components=2, random_state=0, perplexity=30)\n",
    "reduced_embeddings = tsne.fit_transform(embeddings)\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "for i, label in enumerate(vocab):\n",
    "    x, y = reduced_embeddings[i, :]\n",
    "    plt.scatter(x, y)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "###################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.Visualise the Word Representations of Biomedical Entities by t-SNE\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import scispacy\n",
    "import spacy\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "\n",
    "nlp = spacy.load(\"en_core_sci_sm\")\n",
    "\n",
    "def extract_biomedical_entities(text_data):\n",
    "    entities = defaultdict(list)\n",
    "    for doc in nlp.pipe(text_data):\n",
    "        for entity in doc.ents:\n",
    "            entities[entity.text].append(entity.label_)\n",
    "    return entities\n",
    "\n",
    "biomedical_entities = extract_biomedical_entities(text_data)\n",
    "vocab = set(skipgram_model.wv.index_to_key)\n",
    "biomedical_entities_in_vocab = {entity for entity in biomedical_entities if entity in vocab}\n",
    "\n",
    "# Extract embeddings for the entities\n",
    "biomedical_entity_vectors = np.array([skipgram_model.wv[entity] for entity in biomedical_entities_in_vocab])\n",
    "\n",
    "\n",
    "max_entities = 500\n",
    "selected_indices = np.random.choice(len(biomedical_entity_vectors), max_entities, replace=False)\n",
    "biomedical_entity_vectors = biomedical_entity_vectors[selected_indices]\n",
    "biomedical_entities_in_vocab = [list(biomedical_entities_in_vocab)[i] for i in selected_indices]\n",
    "\n",
    "def visualize_biomedical_entities(embeddings, labels):\n",
    "    tsne = TSNE(n_components=2, random_state=0)\n",
    "    reduced_embeddings = tsne.fit_transform(embeddings)\n",
    "\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    for i, label in enumerate(labels):\n",
    "        x, y = reduced_embeddings[i, :]\n",
    "        plt.scatter(x, y)\n",
    "        plt.text(x + 0.1, y + 0.1, label, fontsize=9)\n",
    "    plt.show()\n",
    "\n",
    "# Visualize the biomedical entities\n",
    "visualize_biomedical_entities(biomedical_entity_vectors, list(biomedical_entities_in_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3.Co-occurrence with biomedical words\n",
    "import numpy as np\n",
    "\n",
    "# Co-occurrence Analysis\n",
    "def co_occurrence(tokens, target_word):\n",
    "    co_occur_dict = {}\n",
    "    for token_list in tokens:\n",
    "        if target_word in token_list:\n",
    "            for token in token_list:\n",
    "                if token != target_word:\n",
    "                    co_occur_dict[token] = co_occur_dict.get(token, 0) + 1\n",
    "    return co_occur_dict\n",
    "\n",
    "co_occurrences = co_occurrence(biomedical_entities, 'COVID-19')\n",
    "print(sorted(co_occurrences.items(), key=lambda x: x[1], reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4.Semantic Similarity with biomedical words\n",
    "def get_most_similar_words(model, target_word, top_n=10):\n",
    "    similar_words = model.wv.most_similar(target_word, topn=top_n)\n",
    "    return similar_words\n",
    "\n",
    "similar_words = get_most_similar_words(biomedical_entity_vectors, 'COVID_19')\n",
    "print(similar_words)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
